{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:19.097338Z",
     "start_time": "2025-06-16T14:11:16.149803Z"
    }
   },
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 奖励模型",
   "id": "7d02c9b935543a53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:23.653427Z",
     "start_time": "2025-06-16T14:11:19.104331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reward_tokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/reward-model-deberta-v3-base\")\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"OpenAssistant/reward-model-deberta-v3-base\")\n",
    "\n",
    "device = torch.device('mps')\n",
    "reward_model = reward_model.to(device)\n",
    "reward_model.eval()"
   ],
   "id": "3b692fe73c4f18f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:23.676087Z",
     "start_time": "2025-06-16T14:11:23.668862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Please tell me how to learn machine learning.\"\n",
    "response1 = \"Learning machine learning can start with mathematical foundations, then learn programming, followed by learning core algorithms, and through project practice.\"\n",
    "response2 = \"I really like to eat chocolate.\"\n",
    "response3 = \"Learning machine learning can start with eat apples, then oranges, followed by jumping into a swimming pool.\"\n",
    "\n",
    "responses = [response1, response2, response3]\n",
    "prompts = [prompt] * 3\n",
    "encoded_input = reward_tokenizer(\n",
    "    prompts,\n",
    "    responses,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\", # 返回 PyTorch 张量\n",
    "    max_length = 512\n",
    ").to(device)\n",
    "print(encoded_input.keys())\n",
    "print(encoded_input['input_ids'].tolist()[0])"
   ],
   "id": "b0dfc5168b1d71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[1, 863, 848, 351, 361, 264, 799, 1494, 1101, 260, 2, 4735, 1494, 1101, 295, 564, 275, 10378, 10355, 261, 393, 799, 4050, 261, 1708, 293, 1101, 2233, 8785, 261, 263, 390, 663, 1105, 260, 2]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:23.949168Z",
     "start_time": "2025-06-16T14:11:23.687006Z"
    }
   },
   "cell_type": "code",
   "source": "scores = reward_model(**encoded_input)",
   "id": "16a0fe1241b03d46",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:24.016317Z",
     "start_time": "2025-06-16T14:11:23.955858Z"
    }
   },
   "cell_type": "code",
   "source": "scores.logits.squeeze(-1)",
   "id": "6db1dae80bdc8782",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0531, -5.3392, -4.6690], device='mps:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Actor-Critic 网络",
   "id": "7ff96a14d53c8b66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:24.025284Z",
     "start_time": "2025-06-16T14:11:24.021391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "class PolicyWithValueHeadWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper around a Causal Language Model to add a Value Head.\n",
    "    Mirrors the forward signature needed for generation and PPO loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: AutoModelForCausalLM):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        # Get the hidden size from the base model's config\n",
    "        config = base_model.config\n",
    "        if hasattr(config, 'hidden_size'):\n",
    "            hidden_size = config.hidden_size\n",
    "        elif hasattr(config, 'n_embd'):\n",
    "            hidden_size = config.n_embd\n",
    "        else:\n",
    "             raise ValueError(f\"Cannot find hidden size in config: {config}\")\n",
    "        self.v_head = nn.Linear(hidden_size, 1)\n",
    "        nn.init.normal_(self.v_head.weight, std=0.01)\n",
    "        nn.init.zeros_(self.v_head.bias)\n",
    "    # Define forward method matching the base model's common signature\n",
    "    # This signature is crucial for compatibility with generate and PPO logic\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, # Use standard tuple hint\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        # **kwargs # Allow for other potential arguments from different models\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[Tuple[Tuple[torch.Tensor]]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Input token IDs.\n",
    "            attention_mask: Attention mask.\n",
    "            past_key_values: KV cache from previous steps.\n",
    "            use_cache: Whether to return updated KV cache.\n",
    "            ... other args passed to the base model.\n",
    "        Returns:\n",
    "            logits: Logits from the language model head (batch_size, seq_len, vocab_size).\n",
    "            values: Value predictions from the value head (batch_size, seq_len).\n",
    "            past_key_values: Updated KV cache.\n",
    "        \"\"\"\n",
    "        # Ensure output_hidden_states is True to get necessary input for v_head\n",
    "        # Call base model forward\n",
    "        outputs: CausalLMOutputWithPast = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        values_full_seq_chunk = self.v_head(last_hidden_state).squeeze(-1)\n",
    "        return outputs.logits, values_full_seq_chunk, outputs.past_key_values"
   ],
   "id": "f213ecda242867e1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:28.942901Z",
     "start_time": "2025-06-16T14:11:24.035143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载模型和分词器\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.pad_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = '[pad]'\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "policy_model = PolicyWithValueHeadWrapper(base_model)\n",
    "policy_model.to(device)\n",
    "policy_model"
   ],
   "id": "d9ac9867463f6f93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PolicyWithValueHeadWrapper(\n",
       "  (base_model): Qwen3ForCausalLM(\n",
       "    (model): Qwen3Model(\n",
       "      (embed_tokens): Embedding(151936, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen3DecoderLayer(\n",
       "          (self_attn): Qwen3Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Qwen3MLP(\n",
       "            (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      (rotary_emb): Qwen3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "  )\n",
       "  (v_head): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据收集\n",
    "从 Prompt 数据集中获取一批 Prompts，使用当前的 policy_model 为每个 Prompt 生成 Responses。"
   ],
   "id": "4af94bcf78c882ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:28.955536Z",
     "start_time": "2025-06-16T14:11:28.951680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DynamicCache\n",
    "\n",
    "def apply_top_k_filter(logits: torch.Tensor, top_k: int):\n",
    "    if top_k <= 0:\n",
    "        return logits\n",
    "\n",
    "    top_k_values, _ = torch.topk(logits, top_k, dim=-1)\n",
    "    kth_value = top_k_values[:, -1].unsqueeze(-1)\n",
    "    indices_to_remove = logits < kth_value\n",
    "    logits[indices_to_remove] = -float('inf')\n",
    "    return logits\n",
    "\n",
    "def apply_top_p_filtering(logits: torch.Tensor, top_p: float):\n",
    "    if top_p >= 1.0:\n",
    "        return logits\n",
    "\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=True)\n",
    "    sorted_probs = nn.functional.softmax(sorted_logits, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    indices_to_remove = cumulative_probs > top_p\n",
    "    indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "    indices_to_remove[..., 0] = False\n",
    "    mask_in_original_order = torch.zeros_like(logits, dtype=torch.bool, device=logits.device)\n",
    "    mask_in_original_order.scatter_(dim=-1, index=sorted_indices, src=indices_to_remove)\n",
    "    logits[mask_in_original_order] = -float('inf')\n",
    "    return logits"
   ],
   "id": "4a6d42c3302dda9f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:28.976253Z",
     "start_time": "2025-06-16T14:11:28.964854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DynamicCache\n",
    "from torch import nn\n",
    "\n",
    "def generate_response_with_probs_values_batch(model: nn.Module, tokenizer, prompts, device, max_new_tokens=50, temperature=1.0, top_k=None, top_p=None, do_sample=True):\n",
    "    model.eval()\n",
    "    batch_size = len(prompts)\n",
    "\n",
    "    tokenized_prompts = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "    input_ids = tokenized_prompts.input_ids.to(device)\n",
    "    attention_mask = tokenized_prompts.attention_mask.to(device)\n",
    "\n",
    "    prompt_lengths = attention_mask.sum(dim=1)\n",
    "    max_prompt_length = input_ids.size(1)\n",
    "    max_total_length = max_prompt_length + max_new_tokens\n",
    "\n",
    "    response_ids = torch.full((batch_size, max_new_tokens), tokenizer.pad_token_id, dtype=torch.long, device=device)\n",
    "    log_probs = torch.full((batch_size, max_new_tokens), 0.0, dtype=torch.float, device=device)\n",
    "    batch_values_padded = torch.full((batch_size, max_new_tokens + 1), 0.0, dtype=torch.float, device=device)\n",
    "\n",
    "    is_finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "    sequence_lengths = prompt_lengths.clone()\n",
    "    current_response_length = 0 # Number of response tokens generated so far (0 to max_new_tokens-1)\n",
    "    past_key_values = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # First call: Process the full prompt\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=None,\n",
    "            use_cache=True\n",
    "        )\n",
    "        # logits (bs, prompt_len, vs), values_chunk (bs, prompt_len), past_kv (DynamicCache)\n",
    "        logits, values_chunk_prompt, past_key_values = outputs\n",
    "\n",
    "    batch_values_padded[:, 0] = values_chunk_prompt[:, -1] # Shape (batch_size,)\n",
    "    last_token_logits = logits[torch.arange(batch_size, device=device), prompt_lengths - 1, :] # (1, vocab_size)\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        if torch.all(is_finished):\n",
    "            break\n",
    "\n",
    "        current_logits = last_token_logits\n",
    "\n",
    "        if temperature != 1.0:\n",
    "            if temperature < 0:\n",
    "                temperature = 1.0\n",
    "            current_logits = current_logits / temperature\n",
    "\n",
    "        log_probs_dist = nn.functional.log_softmax(current_logits, dim=-1)  # (batch_size, vocab_size)\n",
    "\n",
    "        if do_sample:\n",
    "            filtered_logits = current_logits.clone()\n",
    "            if top_k is not None and top_k > 0:\n",
    "                 filtered_logits = apply_top_k_filter(filtered_logits, top_k)\n",
    "            if top_p is not None and top_p < 1.0:\n",
    "                 filtered_logits = apply_top_p_filtering(filtered_logits, top_p)\n",
    "            probs = torch.softmax(filtered_logits, dim=-1)  # (batch_size, vocab_size)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "        else:\n",
    "            next_tokens = torch.argmax(current_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        sampled_token_log_probs = log_probs_dist.gather(1, next_tokens).squeeze(1) # (batch_size,)\n",
    "\n",
    "        active_sequences = ~is_finished\n",
    "\n",
    "        response_ids[active_sequences, current_response_length] = next_tokens[active_sequences].squeeze(1)\n",
    "        log_probs[active_sequences, current_response_length] = sampled_token_log_probs[active_sequences]\n",
    "\n",
    "        sequence_lengths[active_sequences] += 1\n",
    "\n",
    "        # 生成是否结束的mask\n",
    "        current_tokens_are_eos = (next_tokens.squeeze(1) == tokenizer.eos_token_id)\n",
    "        reached_max_len = (sequence_lengths >= max_total_length)\n",
    "        is_finished = is_finished | current_tokens_are_eos | reached_max_len\n",
    "\n",
    "        # 生成下一个seq_len前向传播的数据\n",
    "        current_input_ids_for_next_step = next_tokens\n",
    "        current_total_length_for_mask = max_prompt_length + step + 1\n",
    "        new_attention_mask = torch.zeros(batch_size, current_total_length_for_mask, dtype=torch.long, device=device)\n",
    "        for i in range(batch_size):\n",
    "             new_attention_mask[i, :sequence_lengths[i]] = 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 调用 policy_model 并获取 logits 和 value\n",
    "            outputs = model(\n",
    "                input_ids=current_input_ids_for_next_step,\n",
    "                attention_mask=new_attention_mask,\n",
    "                past_key_values=DynamicCache.from_legacy_cache(past_key_values),\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits, values_chunk_step, past_key_values = outputs\n",
    "\n",
    "        batch_values_padded[:, step + 1] = values_chunk_step.squeeze(1)\n",
    "        last_token_logits = logits.squeeze(1)\n",
    "\n",
    "        current_response_length += 1\n",
    "\n",
    "    # 获取生成数据\n",
    "    collected_data = {\n",
    "        'prompts': [],\n",
    "        'responses': [],\n",
    "        'full_sequence_ids': [],\n",
    "        'response_ids': [],\n",
    "        'log_probs': [],\n",
    "        'values': [],\n",
    "        'actual_response_length': []\n",
    "    }\n",
    "    for i in range(batch_size):\n",
    "        prompt_ids = tokenizer(prompts[i], return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        seq_generated_response_ids = response_ids[i]\n",
    "        actual_response_length = (seq_generated_response_ids != tokenizer.pad_token_id).sum().item() # Count non-pad tokens\n",
    "\n",
    "        actual_response_ids = seq_generated_response_ids[:actual_response_length]\n",
    "        full_sequence_ids = torch.cat([prompt_ids.squeeze(0), actual_response_ids], dim=0)\n",
    "        response_text = tokenizer.decode(actual_response_ids, skip_special_tokens=True)\n",
    "\n",
    "        collected_data['prompts'].append(prompts[i])\n",
    "        collected_data['responses'].append(response_text)\n",
    "        collected_data['full_sequence_ids'].append(full_sequence_ids)\n",
    "        collected_data['response_ids'].append(actual_response_ids)\n",
    "        collected_data['log_probs'].append(log_probs[i][:actual_response_length])\n",
    "        collected_data['values'].append(batch_values_padded[i, :actual_response_length + 1])\n",
    "        collected_data['actual_response_length'].append(actual_response_length)\n",
    "    return collected_data\n"
   ],
   "id": "3a13fd1b02ce3b67",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:11:53.464041Z",
     "start_time": "2025-06-16T14:11:28.980226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_prompts = [\n",
    "        \"写一个关于机器学习的短介绍。\",\n",
    "        \"如何用Python实现一个简单的线性回归模型？\",\n",
    "    ]\n",
    "\n",
    "rollout_data_batch = generate_response_with_probs_values_batch(\n",
    "    policy_model, tokenizer, test_prompts, device, max_new_tokens=200, temperature=1.0, top_k=5, top_p=0.8\n",
    ")"
   ],
   "id": "b24fa37ac8ef7575",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 收集奖励",
   "id": "e9f204b4ae1c5aa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:00.199202Z",
     "start_time": "2025-06-16T14:11:53.468298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ref_base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_model = PolicyWithValueHeadWrapper(ref_base_model).to(device)"
   ],
   "id": "db5f8b382afd75a2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:00.222157Z",
     "start_time": "2025-06-16T14:12:00.210248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_value, dtype=None, device=None):\n",
    "    if not list_of_tensors:\n",
    "        return torch.empty(0, dtype=dtype, device=device), torch.empty(0, dtype=torch.bool, device=device)\n",
    "\n",
    "    max_len = max(t.size(0) for t in list_of_tensors)\n",
    "    padded_tensors = []\n",
    "    masks = []\n",
    "\n",
    "    if dtype is None:\n",
    "        dtype = list_of_tensors[0].dtype\n",
    "    if device is None:\n",
    "        device = list_of_tensors[0].device\n",
    "\n",
    "    for t in list_of_tensors:\n",
    "        pad_len = max_len - t.size(0)\n",
    "        padded_t = nn.functional.pad(t, (0, pad_len), value=pad_value)\n",
    "        padded_tensors.append(padded_t)\n",
    "\n",
    "        mask = torch.ones(t.size(0), dtype=torch.bool, device=device)\n",
    "        mask = nn.functional.pad(mask, (0, pad_len), value=False)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return torch.stack(padded_tensors), torch.stack(masks)\n",
    "\n",
    "def get_reward_scores(data, ref_model, reward_model, tokenizer, reward_tokenizer, kl_beta=0.01, device=None):\n",
    "    batch_size = len(data['prompts'])\n",
    "\n",
    "    prompts = data['prompts']\n",
    "    response_texts = data['responses']\n",
    "    list_full_sequence_ids = data['full_sequence_ids']\n",
    "    list_response_ids = data['response_ids']\n",
    "    list_policy_log_probs = data['log_probs']\n",
    "    list_values = data['values']\n",
    "    actual_response_lengths = data['actual_response_length']\n",
    "\n",
    "    # 1.对收集数据进行pad\n",
    "    # pad full_sequence_id\n",
    "    padded_full_sequence_ids, attention_mask_full_sequence = pad_list_of_tensors(\n",
    "        list_full_sequence_ids,\n",
    "        pad_value=tokenizer.pad_token_id,\n",
    "        dtype=torch.long,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # pad response_id\n",
    "    padded_response_ids, response_mask = pad_list_of_tensors(\n",
    "        list_response_ids,\n",
    "        pad_value=tokenizer.pad_token_id,\n",
    "        dtype=torch.long,\n",
    "        device=device\n",
    "    )\n",
    "    max_response_len = padded_response_ids.size(1)\n",
    "\n",
    "    # pad policy_log_probs\n",
    "    padded_policy_log_probs, _ = pad_list_of_tensors(\n",
    "        list_policy_log_probs,\n",
    "        pad_value=0.0,\n",
    "        dtype=torch.float,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # pad values\n",
    "    # padded_values.size(1) = max_response_len + 1 : 对prompt的估计作为第1个元素\n",
    "    padded_values, _ = pad_list_of_tensors(\n",
    "        list_values,\n",
    "        pad_value=0.0,\n",
    "        dtype=torch.float,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    actual_prompt_lengths = torch.tensor([len(list_full_sequence_ids[i]) - actual_response_lengths[i] for i in range(batch_size)], device=device)\n",
    "\n",
    "    # 2.计算奖励分数\n",
    "    reward_model_inputs = [p + r for p, r in zip(prompts, response_texts)]\n",
    "    tokenized_reward_inputs = reward_tokenizer(\n",
    "        reward_model_inputs,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reward_outputs = reward_model(**tokenized_reward_inputs)\n",
    "        reward_scores = reward_outputs.logits.squeeze(-1)\n",
    "\n",
    "    # 3.计算KL散度\n",
    "    with torch.no_grad():\n",
    "        ref_outputs = ref_model(\n",
    "            input_ids=padded_full_sequence_ids,\n",
    "            attention_mask=attention_mask_full_sequence,\n",
    "            return_dict=True\n",
    "        )\n",
    "        ref_logits = ref_outputs[0]\n",
    "\n",
    "    response_step_indices = torch.arange(max_response_len, device=device).unsqueeze(0).expand(batch_size, -1) # Shape: (batch_size, max_response_len)\n",
    "\n",
    "    expanded_prompt_lengths = actual_prompt_lengths.unsqueeze(-1).expand(-1, max_response_len)\n",
    "    context_indices_in_full_logits = expanded_prompt_lengths + response_step_indices - 1\n",
    "    context_indices_expanded = context_indices_in_full_logits.unsqueeze(-1).expand(-1, -1, ref_logits.size(-1))\n",
    "\n",
    "    # 保留response部分的logits\n",
    "    ref_logits_for_response_context = torch.gather(ref_logits, dim=1, index=context_indices_expanded)\n",
    "    ref_log_probs_dist = nn.functional.log_softmax(ref_logits_for_response_context, dim=-1)\n",
    "\n",
    "    # 得到response_ids部分对应的的logits\n",
    "    ref_log_probs_generated = ref_log_probs_dist.gather(2, padded_response_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    kl_per_token = padded_policy_log_probs - ref_log_probs_generated\n",
    "    masked_kl_per_token = kl_per_token * response_mask.float()\n",
    "    total_kl_divergence = masked_kl_per_token.sum(dim=-1)\n",
    "\n",
    "    # 4.最终总奖励\n",
    "    rewards = reward_scores - kl_beta * total_kl_divergence\n",
    "\n",
    "    return {\n",
    "        \"rewards\": rewards,                    # Shape: (batch_size,)\n",
    "        \"padded_values\": padded_values,              # Shape: (batch_size, max_response_len + 1)\n",
    "        \"padded_policy_log_probs\": padded_policy_log_probs,    # Shape: (batch_size, max_response_len)\n",
    "        \"padded_response_ids\": padded_response_ids,        # Shape: (batch_size, max_response_len)\n",
    "        \"padded_full_sequence_ids\": padded_full_sequence_ids,   # Shape: (batch_size, max_total_len)\n",
    "        \"attention_mask_full_sequence\": attention_mask_full_sequence, # Shape: (batch_size, max_total_len)\n",
    "        \"response_mask\": response_mask               # Shape: (batch_size, max_response_len)\n",
    "}"
   ],
   "id": "b003d863017c62b1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:01.879599Z",
     "start_time": "2025-06-16T14:12:00.232227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = get_reward_scores(rollout_data_batch, ref_model, reward_model, tokenizer, reward_tokenizer, device=device)\n",
    "print(res['rewards'])"
   ],
   "id": "fce0cf7188008f2b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-7.7583, -6.3136], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 计算GAE",
   "id": "9f32a34073a7478a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:01.896113Z",
     "start_time": "2025-06-16T14:12:01.891571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_gae(rewards, padded_values, response_mask, device, gamma=0.99, lambda_=0.95):\n",
    "    batch_size, max_response_len = response_mask.shape\n",
    "\n",
    "    rewards = rewards.to(device)\n",
    "    padded_values = padded_values.to(device)\n",
    "    response_mask = response_mask.to(device)\n",
    "\n",
    "    rewards_padded_per_step = torch.zeros(batch_size, max_response_len, dtype=torch.float, device=device)\n",
    "\n",
    "    actual_response_lengths = response_mask.sum(dim=1)\n",
    "    valid_indices = actual_response_lengths > 0\n",
    "    if torch.any(valid_indices):\n",
    "        last_valid_response_indices = actual_response_lengths[valid_indices] - 1\n",
    "        batch_indices = torch.arange(batch_size, device=device)[valid_indices]\n",
    "        rewards_padded_per_step[batch_indices, last_valid_response_indices] = rewards[valid_indices]\n",
    "\n",
    "    # 优势计算\n",
    "    delta = rewards_padded_per_step + gamma * padded_values[:, 1:] - padded_values[:, :-1]\n",
    "    advantages = torch.zeros_like(delta, device=device)\n",
    "    last_gae_lambda = torch.zeros(batch_size, device=device)\n",
    "\n",
    "    for t in reversed(range(max_response_len)):\n",
    "        delta_t = delta[:, t]\n",
    "        masked_delta_t = delta_t * response_mask[:, t].float()\n",
    "        advantages_t = masked_delta_t + gamma * lambda_ * last_gae_lambda\n",
    "        advantages[:, t] = advantages_t\n",
    "        last_gae_lambda = advantages_t\n",
    "\n",
    "    advantages = advantages * response_mask.float()\n",
    "    returns = advantages + padded_values[:, :-1]\n",
    "    return advantages, returns"
   ],
   "id": "77bfac77cc796bf5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:02.195794Z",
     "start_time": "2025-06-16T14:12:01.915812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "advantages, returns = calculate_gae(res['rewards'], res['padded_values'], res['response_mask'], device)\n",
    "print(advantages)\n",
    "print(returns)"
   ],
   "id": "e1a50237f5d38909",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.7968e-01, -9.6657e-02,  6.4990e-01,  7.2113e-01, -3.9091e-01,\n",
      "          3.4005e-02, -4.1875e-01, -1.3563e-01, -3.6192e-01,  3.7891e-02,\n",
      "         -2.3264e-01,  3.5953e-01,  3.5520e-01,  1.0672e-02,  2.7089e-01,\n",
      "          1.3855e-01,  1.4101e-01, -3.0462e-01,  1.4752e-01,  8.3769e-02,\n",
      "          1.0854e-01, -1.2197e-01, -3.2931e-01, -5.7190e-01,  1.8100e-01,\n",
      "         -2.0163e-01,  4.6049e-02, -3.2317e-01, -8.7065e-02,  1.2481e-01,\n",
      "          1.8563e-01, -1.7840e-01,  6.1262e-01, -3.3524e-01,  3.9890e-01,\n",
      "          1.5589e-01, -6.8010e-02, -1.8405e-01,  2.1685e-01,  3.4398e-02,\n",
      "          1.5487e-01, -5.3448e-01, -6.0253e-02,  1.0317e-01,  2.4802e-01,\n",
      "         -3.1722e-01,  2.7089e-01, -4.1476e-01, -2.0222e-01, -1.8442e-01,\n",
      "         -4.2875e-03, -1.5608e-01, -4.1056e-01, -1.2294e-02, -2.5024e-02,\n",
      "          1.8066e-01, -2.7647e-01, -1.0028e-01, -6.6354e-01, -3.4547e-01,\n",
      "          8.4479e-02, -2.2000e-01,  1.2214e-01, -1.4676e-01,  3.2425e-01,\n",
      "         -7.1166e-02,  1.5201e-01,  3.1080e-02, -1.9391e-01,  6.3129e-02,\n",
      "          8.8705e-02, -5.0601e-01,  6.8945e-02,  1.6715e-01, -2.1713e-01,\n",
      "          5.5044e-02,  3.6239e-01, -3.3415e-02,  3.4347e-01,  1.1969e-01,\n",
      "          1.1320e-01,  5.0110e-02,  1.8287e-01, -6.8772e-02, -2.3647e-01,\n",
      "          1.3115e-01, -1.5496e-01,  7.0966e-02, -1.3581e-01,  6.9503e-03,\n",
      "         -3.6127e-01, -1.4644e-01, -3.6752e-01, -1.8311e-01, -5.3756e-03,\n",
      "          9.2801e-02, -1.4165e-02,  1.7572e-01, -2.0635e-01, -1.8704e-02,\n",
      "         -1.1930e-01,  2.9307e-02, -1.4194e-02,  2.7845e-02, -3.5344e-01,\n",
      "         -1.8179e-01, -3.2167e-01,  2.0082e-01, -4.8679e-01,  1.8732e-01,\n",
      "          7.6313e-02, -3.5131e-01, -6.2612e-02,  7.3860e-02, -5.5679e-02,\n",
      "         -1.8272e-02, -8.3242e-02, -1.0141e-01, -1.6907e-01,  1.2094e-01,\n",
      "         -1.8573e-01, -1.7184e-01, -1.5087e-01,  1.8890e-01, -1.5068e-02,\n",
      "          1.0189e-01,  1.5670e-01,  1.8093e-01,  2.8049e-02, -1.1566e-02,\n",
      "         -1.2927e-01, -1.6122e-01, -1.5010e-01, -1.6476e-01, -1.5700e-01,\n",
      "         -8.8861e-02, -2.8431e-01, -3.2092e-01, -3.4959e-01, -2.4522e-01,\n",
      "         -1.2984e-01, -2.3567e-01, -2.8397e-01, -3.3936e-01, -7.9721e-02,\n",
      "         -1.5105e-01, -5.3011e-02, -1.5830e-01, -2.8519e-01, -3.2443e-01,\n",
      "         -2.6859e-01, -3.6563e-01, -3.7066e-01, -3.9654e-01, -4.1610e-01,\n",
      "         -4.8646e-01, -5.1661e-01, -5.8109e-01, -6.1917e-01, -6.4416e-01,\n",
      "         -6.4484e-01, -7.3514e-01, -8.1715e-01, -8.4897e-01, -9.0543e-01,\n",
      "         -9.2048e-01, -1.0562e+00, -1.2139e+00, -1.2904e+00, -1.2088e+00,\n",
      "         -1.2951e+00, -1.4170e+00, -1.5218e+00, -1.5739e+00, -1.6184e+00,\n",
      "         -1.7640e+00, -1.9680e+00, -2.0219e+00, -2.1394e+00, -2.3394e+00,\n",
      "         -2.4937e+00, -2.6489e+00, -2.8235e+00, -2.9434e+00, -3.1136e+00,\n",
      "         -3.3006e+00, -3.5645e+00, -3.7799e+00, -4.0357e+00, -4.2722e+00,\n",
      "         -4.5073e+00, -4.7727e+00, -5.0973e+00, -5.4481e+00, -5.8046e+00,\n",
      "         -6.1789e+00, -6.4899e+00, -6.8974e+00, -7.3365e+00, -7.8332e+00],\n",
      "        [-1.0528e+00, -1.2498e-01,  9.2359e-01, -1.4025e-02,  7.8126e-01,\n",
      "         -7.6139e-01,  3.9573e-01, -6.1053e-01, -2.5017e-02,  1.2961e-01,\n",
      "          1.9014e-01,  6.7514e-02, -2.7920e-01, -3.5329e-01, -2.3020e-01,\n",
      "         -4.1595e-01,  1.1752e+00, -4.4907e-01, -1.8137e-01,  8.9333e-01,\n",
      "         -4.4988e-01, -1.8134e-01,  2.2667e-01, -1.2862e-01,  2.5345e-01,\n",
      "          3.7786e-01,  5.6624e-02, -2.9520e-01, -1.5849e-01, -6.2332e-01,\n",
      "          6.4339e-01, -3.3781e-01,  7.7153e-02,  1.7259e-01, -1.5349e-01,\n",
      "          3.0694e-01,  4.1723e-01,  6.6763e-02, -5.2809e-02, -1.2983e-01,\n",
      "         -6.2761e-01,  7.0182e-01, -3.0770e-01,  7.2133e-02,  1.7074e-01,\n",
      "         -1.0113e-01,  3.7625e-01,  5.3654e-01,  9.3508e-02, -3.0698e-02,\n",
      "         -2.0279e-01, -6.8899e-01,  7.0178e-01, -2.3465e-01,  7.5297e-02,\n",
      "          1.3096e-01, -4.6527e-02,  3.6383e-01,  5.1996e-01,  8.0250e-02,\n",
      "         -4.0448e-01, -1.5592e-01, -7.0862e-01,  6.3443e-01, -1.4892e-01,\n",
      "         -3.3188e-02,  9.9938e-02, -1.2321e-01,  3.6127e-01,  5.0591e-01,\n",
      "          6.4644e-02, -7.3062e-01, -1.5071e-01, -7.0185e-01,  6.1195e-01,\n",
      "         -2.0555e-01, -1.1487e-01, -9.5273e-02, -2.2479e-01,  3.2253e-01,\n",
      "          5.2864e-01,  7.3242e-02, -1.6556e-01, -2.0823e-01, -7.2744e-01,\n",
      "          5.5822e-01, -2.3474e-01, -1.4033e-01,  8.5485e-03, -2.0961e-01,\n",
      "          3.5223e-01,  5.6633e-01,  1.1285e-01, -8.3026e-01, -2.2615e-01,\n",
      "         -7.4674e-01,  5.3045e-01, -2.5267e-01, -1.8899e-01, -7.6062e-02,\n",
      "         -2.7092e-01,  3.1424e-01,  5.1420e-01,  6.1105e-02, -5.3538e-01,\n",
      "         -2.3194e-01, -7.6216e-01,  5.0072e-01, -1.9318e-01, -2.2546e-01,\n",
      "         -6.9622e-02, -3.0764e-01,  2.8426e-01,  4.8470e-01,  3.0734e-02,\n",
      "         -2.4808e-01, -3.0544e-01, -7.8499e-01,  4.4740e-01, -1.6935e-01,\n",
      "         -2.9429e-01, -1.1980e-01, -3.3779e-01,  2.7090e-01,  4.6459e-01,\n",
      "         -1.9294e-02, -7.0486e-01, -3.1149e-01, -8.1141e-01,  3.8479e-01,\n",
      "         -2.6760e-01, -3.3145e-01, -1.7606e-01, -3.8216e-01,  2.0855e-01,\n",
      "          4.0062e-01, -6.7390e-02, -8.5660e-01, -4.0014e-01, -9.0989e-01,\n",
      "          2.9625e-01, -4.0024e-01, -4.0383e-01, -2.7910e-01, -4.9238e-01,\n",
      "          7.1682e-02,  2.6966e-01, -2.2152e-01, -1.0393e+00, -5.4338e-01,\n",
      "         -1.0940e+00,  7.2213e-02, -6.4111e-01, -6.3956e-01, -5.1785e-01,\n",
      "         -7.1237e-01, -1.8519e-01, -1.6129e-02, -5.1919e-01, -8.2871e-01,\n",
      "         -8.3072e-01, -1.3574e+00, -2.5008e-01, -9.9542e-01, -1.0035e+00,\n",
      "         -9.3480e-01, -1.1395e+00, -6.3735e-01, -4.7406e-01, -1.0224e+00,\n",
      "         -1.5394e+00, -1.4471e+00, -1.9899e+00, -8.8554e-01, -1.7158e+00,\n",
      "         -1.7456e+00, -1.7362e+00, -1.9943e+00, -1.4881e+00, -1.4171e+00,\n",
      "         -2.0046e+00, -2.8267e+00, -2.6365e+00, -3.2392e+00, -2.2194e+00,\n",
      "         -3.0981e+00, -3.1810e+00, -3.3277e+00, -3.6840e+00, -3.2807e+00,\n",
      "         -3.3265e+00, -4.0055e+00, -4.5355e+00, -4.7630e+00, -5.5950e+00,\n",
      "         -4.7547e+00, -5.7773e+00, -6.0257e+00, -6.3651e+00, -6.9179e+00]],\n",
      "       device='mps:0')\n",
      "tensor([[ 9.0114e-02,  8.6192e-02,  1.1956e-01,  1.5682e-01,  1.3886e-01,\n",
      "          1.4196e-01,  1.2246e-01,  1.1691e-01,  1.0000e-01,  1.0290e-01,\n",
      "          9.2312e-02,  1.1122e-01,  1.3010e-01,  1.3195e-01,  1.4683e-01,\n",
      "          1.5524e-01,  1.6386e-01,  1.5028e-01,  1.5918e-01,  1.6497e-01,\n",
      "          1.7207e-01,  1.6771e-01,  1.5293e-01,  1.2588e-01,  1.3621e-01,\n",
      "          1.2750e-01,  1.3109e-01,  1.1626e-01,  1.1308e-01,  1.2046e-01,\n",
      "          1.3096e-01,  1.2336e-01,  1.5524e-01,  1.4004e-01,  1.6140e-01,\n",
      "          1.7083e-01,  1.6915e-01,  1.6166e-01,  1.7413e-01,  1.7761e-01,\n",
      "          1.8715e-01,  1.6232e-01,  1.6094e-01,  1.6773e-01,  1.8182e-01,\n",
      "          1.6780e-01,  1.8304e-01,  1.6415e-01,  1.5570e-01,  1.4805e-01,\n",
      "          1.4933e-01,  1.4303e-01,  1.2395e-01,  1.2459e-01,  1.2460e-01,\n",
      "          1.3489e-01,  1.2243e-01,  1.1865e-01,  8.6671e-02,  7.0273e-02,\n",
      "          7.5207e-02,  6.4966e-02,  7.1729e-02,  6.5116e-02,  8.1986e-02,\n",
      "          7.9256e-02,  8.7657e-02,  9.0096e-02,  8.1311e-02,  8.5288e-02,\n",
      "          9.0585e-02,  6.6200e-02,  7.0316e-02,  7.9383e-02,  6.9329e-02,\n",
      "          7.2781e-02,  9.1636e-02,  9.0891e-02,  1.0898e-01,  1.1607e-01,\n",
      "          1.2290e-01,  1.2665e-01,  1.3707e-01,  1.3502e-01,  1.2456e-01,\n",
      "          1.3237e-01,  1.2596e-01,  1.3078e-01,  1.2531e-01,  1.2692e-01,\n",
      "          1.1014e-01,  1.0393e-01,  8.6608e-02,  7.8327e-02,  7.8849e-02,\n",
      "          8.4286e-02,  8.4429e-02,  9.4068e-02,  8.4701e-02,  8.4621e-02,\n",
      "          7.9511e-02,  8.1779e-02,  8.1895e-02,  8.4115e-02,  6.7293e-02,\n",
      "          5.8883e-02,  4.3394e-02,  5.3873e-02,  3.0078e-02,  3.9748e-02,\n",
      "          4.3965e-02,  2.6843e-02,  2.3984e-02,  2.7919e-02,  2.5417e-02,\n",
      "          2.4760e-02,  2.0848e-02,  1.5988e-02,  7.6958e-03,  1.3820e-02,\n",
      "          4.6735e-03, -3.8712e-03, -1.1454e-02, -2.1243e-03, -2.8991e-03,\n",
      "          2.1663e-03,  1.0023e-02,  1.9171e-02,  2.0767e-02,  2.0399e-02,\n",
      "          1.4141e-02,  6.2229e-03, -1.2193e-03, -9.4698e-03, -1.7415e-02,\n",
      "         -2.2034e-02, -3.6472e-02, -5.2886e-02, -7.0900e-02, -8.3877e-02,\n",
      "         -9.1216e-02, -1.0392e-01, -1.1917e-01, -1.3734e-01, -1.4271e-01,\n",
      "         -1.5171e-01, -1.5589e-01, -1.6538e-01, -1.8131e-01, -1.9936e-01,\n",
      "         -2.1481e-01, -2.3526e-01, -2.5617e-01, -2.7858e-01, -3.0220e-01,\n",
      "         -3.2958e-01, -3.5874e-01, -3.9141e-01, -4.2633e-01, -4.6284e-01,\n",
      "         -4.9976e-01, -5.4156e-01, -5.8789e-01, -6.3628e-01, -6.8798e-01,\n",
      "         -7.4095e-01, -8.0124e-01, -8.7003e-01, -9.4333e-01, -1.0133e+00,\n",
      "         -1.0883e+00, -1.1701e+00, -1.2581e+00, -1.3495e+00, -1.4440e+00,\n",
      "         -1.5468e+00, -1.6608e+00, -1.7787e+00, -1.9036e+00, -2.0398e+00,\n",
      "         -2.1851e+00, -2.3396e+00, -2.5044e+00, -2.6769e+00, -2.8596e+00,\n",
      "         -3.0535e+00, -3.2626e+00, -3.4846e+00, -3.7215e+00, -3.9727e+00,\n",
      "         -4.2382e+00, -4.5197e+00, -4.8202e+00, -5.1413e+00, -5.4835e+00,\n",
      "         -5.8478e+00, -6.2314e+00, -6.6392e+00, -7.0731e+00, -7.5362e+00],\n",
      "        [ 1.0009e-01,  9.4850e-02,  1.4199e-01,  1.4272e-01,  1.8323e-01,\n",
      "          1.4701e-01,  1.6828e-01,  1.3945e-01,  1.3961e-01,  1.4750e-01,\n",
      "          1.5850e-01,  1.6347e-01,  1.5117e-01,  1.3503e-01,  1.2488e-01,\n",
      "          1.0535e-01,  1.6517e-01,  1.4438e-01,  1.3677e-01,  1.8282e-01,\n",
      "          1.6217e-01,  1.5474e-01,  1.6764e-01,  1.6290e-01,  1.7722e-01,\n",
      "          1.9790e-01,  2.0273e-01,  1.9002e-01,  1.8402e-01,  1.5471e-01,\n",
      "          1.8844e-01,  1.7346e-01,  1.7907e-01,  1.8950e-01,  1.8374e-01,\n",
      "          2.0095e-01,  2.2384e-01,  2.2944e-01,  2.2911e-01,  2.2494e-01,\n",
      "          1.9583e-01,  2.3290e-01,  2.1986e-01,  2.2569e-01,  2.3651e-01,\n",
      "          2.3384e-01,  2.5502e-01,  2.8442e-01,  2.9197e-01,  2.9338e-01,\n",
      "          2.8620e-01,  2.5465e-01,  2.9231e-01,  2.8353e-01,  2.9016e-01,\n",
      "          2.9964e-01,  3.0034e-01,  3.2156e-01,  3.5081e-01,  3.5836e-01,\n",
      "          3.4176e-01,  3.3741e-01,  3.0539e-01,  3.4020e-01,  3.3619e-01,\n",
      "          3.3792e-01,  3.4634e-01,  3.4367e-01,  3.6521e-01,  3.9419e-01,\n",
      "          4.0141e-01,  3.6893e-01,  3.6512e-01,  3.3372e-01,  3.6768e-01,\n",
      "          3.6112e-01,  3.5903e-01,  3.5789e-01,  3.5026e-01,  3.6993e-01,\n",
      "          4.0010e-01,  4.0780e-01,  4.0364e-01,  3.9731e-01,  3.6495e-01,\n",
      "          3.9655e-01,  3.8881e-01,  3.8573e-01,  3.9005e-01,  3.8351e-01,\n",
      "          4.0499e-01,  4.3740e-01,  4.4746e-01,  4.1047e-01,  4.0331e-01,\n",
      "          3.7004e-01,  4.0030e-01,  3.9171e-01,  3.8622e-01,  3.8632e-01,\n",
      "          3.7668e-01,  3.9619e-01,  4.2590e-01,  4.3326e-01,  4.1087e-01,\n",
      "          4.0342e-01,  3.6939e-01,  3.9816e-01,  3.9252e-01,  3.8521e-01,\n",
      "          3.8562e-01,  3.7413e-01,  3.9213e-01,  4.2032e-01,  4.2610e-01,\n",
      "          4.1800e-01,  4.0695e-01,  3.7182e-01,  3.9794e-01,  3.9349e-01,\n",
      "          3.8275e-01,  3.8063e-01,  3.6758e-01,  3.8484e-01,  4.1196e-01,\n",
      "          4.1516e-01,  3.8411e-01,  3.7241e-01,  3.3560e-01,  3.5823e-01,\n",
      "          3.4847e-01,  3.3542e-01,  3.3000e-01,  3.1423e-01,  3.2783e-01,\n",
      "          3.5117e-01,  3.5135e-01,  3.1207e-01,  2.9521e-01,  2.5270e-01,\n",
      "          2.7007e-01,  2.5278e-01,  2.3514e-01,  2.2356e-01,  2.0120e-01,\n",
      "          2.0682e-01,  2.2239e-01,  2.1356e-01,  1.6375e-01,  1.3824e-01,\n",
      "          8.4935e-02,  8.9403e-02,  5.8251e-02,  2.6861e-02,  1.2403e-03,\n",
      "         -3.4366e-02, -4.3973e-02, -4.5223e-02, -7.1640e-02, -1.1380e-01,\n",
      "         -1.5648e-01, -2.2593e-01, -2.4072e-01, -2.9292e-01, -3.4606e-01,\n",
      "         -3.9629e-01, -4.5727e-01, -4.9376e-01, -5.2245e-01, -5.7885e-01,\n",
      "         -6.6167e-01, -7.4070e-01, -8.4768e-01, -9.0052e-01, -9.9540e-01,\n",
      "         -1.0927e+00, -1.1906e+00, -1.3023e+00, -1.3899e+00, -1.4748e+00,\n",
      "         -1.5899e+00, -1.7473e+00, -1.8968e+00, -2.0779e+00, -2.2099e+00,\n",
      "         -2.3871e+00, -2.5703e+00, -2.7626e+00, -2.9747e+00, -3.1688e+00,\n",
      "         -3.3671e+00, -3.6014e+00, -3.8646e+00, -4.1418e+00, -4.4633e+00,\n",
      "         -4.7462e+00, -5.0830e+00, -5.4356e+00, -5.8088e+00, -6.2133e+00]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 计算PPO损失",
   "id": "e107555e1fb0a85d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:02.211084Z",
     "start_time": "2025-06-16T14:12:02.204141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_ppo_loss(\n",
    "        policy_model,\n",
    "        padded_policy_log_probs,\n",
    "        padded_response_ids,\n",
    "        padded_full_sequence_ids,\n",
    "        attention_mask_full_sequence,\n",
    "        response_mask,\n",
    "        advantages,\n",
    "        returns,\n",
    "        clip_param,\n",
    "        vf_coef,\n",
    "        ent_coef,\n",
    "        device\n",
    "):\n",
    "    policy_model.train()\n",
    "\n",
    "    batch_size, max_response_len = response_mask.shape\n",
    "    max_total_len = padded_full_sequence_ids.size(1)\n",
    "\n",
    "    max_prompt_length = padded_full_sequence_ids.size(1) - padded_response_ids.size(1)\n",
    "    policy_model.to(device)\n",
    "\n",
    "    # 1.计算模型更新后的logits和values\n",
    "    new_logits_full, new_values_full, _ = policy_model(\n",
    "        input_ids=padded_full_sequence_ids.to(device),\n",
    "        attention_mask=attention_mask_full_sequence.to(device)\n",
    "    )\n",
    "    new_logits_for_response = new_logits_full[:, max_prompt_length - 1 : max_prompt_length - 1 + max_response_len, :]\n",
    "    new_log_softmax = nn.functional.log_softmax(new_logits_for_response, dim=-1)\n",
    "    new_policy_log_probs = new_log_softmax.gather(2, padded_response_ids.to(device).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # 2,PPO策略损失\n",
    "    ratio = torch.exp(new_policy_log_probs - padded_policy_log_probs)\n",
    "    advantages = advantages.to(device)\n",
    "    advantages_flat = advantages[response_mask.to(device)]\n",
    "    if advantages_flat.numel() > 0:\n",
    "        advantages_norm = (advantages - advantages_flat.mean()) / (advantages_flat.std() + 1e-8)\n",
    "    else:\n",
    "         advantages_norm = torch.zeros_like(advantages, device=device)\n",
    "\n",
    "    advantages_norm = advantages_norm * response_mask.float().to(device)\n",
    "\n",
    "    surr1 = ratio * advantages_norm\n",
    "    surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantages_norm\n",
    "    policy_loss_per_token = -torch.min(surr1, surr2)\n",
    "\n",
    "    # 根据长度归一化\n",
    "    num_non_padded_tokens = response_mask.sum()\n",
    "    if num_non_padded_tokens > 0:\n",
    "        policy_loss = (policy_loss_per_token * response_mask.float().to(device)).sum() / num_non_padded_tokens\n",
    "    else:\n",
    "        policy_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # 3.Value loss\n",
    "    value_predictions = new_values_full[:, max_prompt_length - 1: max_prompt_length - 1 + max_response_len]\n",
    "    value_targets= returns.to(device)\n",
    "    value_error = value_predictions - value_targets\n",
    "    value_loss_per_token = value_error.pow(2)\n",
    "\n",
    "    if num_non_padded_tokens > 0:\n",
    "        value_loss = (value_loss_per_token * response_mask.float().to(device)).sum() / num_non_padded_tokens\n",
    "    else:\n",
    "        value_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # 4.交叉熵奖励\n",
    "    probs = torch.exp(new_log_softmax)\n",
    "    entropy_per_token = -(probs * torch.log(probs + 1e-8)).sum(dim=-1)\n",
    "    if num_non_padded_tokens > 0:\n",
    "        entropy = (entropy_per_token * response_mask.float().to(device)).sum() / num_non_padded_tokens\n",
    "    else:\n",
    "        entropy = torch.tensor(0.0, device=device)\n",
    "\n",
    "    total_loss = policy_loss + vf_coef * value_loss - ent_coef * entropy\n",
    "    return total_loss, policy_loss, value_loss, entropy"
   ],
   "id": "369d26531c725a78",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:04.170943Z",
     "start_time": "2025-06-16T14:12:02.223419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_res = res.copy()\n",
    "new_res.pop('rewards')\n",
    "new_res.pop('padded_values')\n",
    "calculate_ppo_loss(\n",
    "    policy_model, advantages=advantages, returns=returns, clip_param=0.2, vf_coef=0.5,\n",
    "    ent_coef=0.05, device=device, **new_res\n",
    ")\n"
   ],
   "id": "7d271b91f452921e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.2228, device='mps:0', grad_fn=<SubBackward0>),\n",
       " tensor(0.0340, device='mps:0', grad_fn=<DivBackward0>),\n",
       " tensor(2.4647, device='mps:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.8727, device='mps:0', grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练数据集",
   "id": "3d9d590513a13606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:04.279872Z",
     "start_time": "2025-06-16T14:12:04.268465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class PPODataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 padded_policy_log_probs: torch.Tensor,\n",
    "                 padded_response_ids: torch.Tensor,\n",
    "                 padded_full_sequence_ids: torch.Tensor,\n",
    "                 attention_mask_full_sequence: torch.Tensor,\n",
    "                 response_mask: torch.Tensor,\n",
    "                 advantages: torch.Tensor,\n",
    "                 returns: torch.Tensor):\n",
    "        self.n_samples = padded_policy_log_probs.size(0)\n",
    "        self.padded_policy_log_probs = padded_policy_log_probs\n",
    "        self.padded_response_ids = padded_response_ids\n",
    "        self.padded_full_sequence_ids = padded_full_sequence_ids\n",
    "        self.attention_mask_full_sequence = attention_mask_full_sequence\n",
    "        self.response_mask = response_mask\n",
    "        self.advantages = advantages\n",
    "        self.returns = returns\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.padded_policy_log_probs[idx],\n",
    "            self.padded_response_ids[idx],\n",
    "            self.padded_full_sequence_ids[idx],\n",
    "            self.attention_mask_full_sequence[idx],\n",
    "            self.response_mask[idx],\n",
    "            self.advantages[idx],\n",
    "            self.returns[idx],\n",
    "        )"
   ],
   "id": "db5cc3493a282a78",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:04.316312Z",
     "start_time": "2025-06-16T14:12:04.303680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def get_prompt_batch(all_prompts, batch_size):\n",
    "    if not all_prompts:\n",
    "        return []\n",
    "    return random.sample(all_prompts, min(batch_size, len(all_prompts)))"
   ],
   "id": "6d4f8be08dd465ce",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练循环",
   "id": "de7156a2d6b89e5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:12:04.349461Z",
     "start_time": "2025-06-16T14:12:04.331471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_ppo(\n",
    "        policy_model: PolicyWithValueHeadWrapper,\n",
    "        ref_model: AutoModelForCausalLM,\n",
    "        reward_model: AutoModelForSequenceClassification,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        reward_tokenizer: AutoTokenizer,\n",
    "        all_prompts_dataset: list[str],\n",
    "        device: torch.device,\n",
    "        epochs: int = 10,\n",
    "        rollout_batch_size: int = 16,\n",
    "        ppo_epochs_per_rollout: int = 4,\n",
    "        minibatch_size: int = 4,\n",
    "        learning_rate: float = 5e-6,\n",
    "        gamma: float = 0.99,\n",
    "        lambda_: float = 0.95,\n",
    "        clip_param: float = 0.2,\n",
    "        vf_coef: float = 0.5,\n",
    "        ent_coef: float = 0.005,\n",
    "        kl_beta: float = 0.1,\n",
    "        max_new_tokens_rollout: int = 50,\n",
    "        temperature: float = 0.7,\n",
    "        top_k: Optional[int] = 50,\n",
    "        top_p: Optional[float] = 0.9,\n",
    "        do_sample: bool = True,\n",
    "):\n",
    "    optimizer = torch.optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{epochs} ---\")\n",
    "\n",
    "        # 收集数据阶段\n",
    "        policy_model.eval()\n",
    "        current_prompts = get_prompt_batch(all_prompts_dataset, rollout_batch_size)\n",
    "        print(current_prompts)\n",
    "        all_rollout_data = generate_response_with_probs_values_batch(\n",
    "            policy_model,\n",
    "            tokenizer,\n",
    "            current_prompts,\n",
    "            device=device,\n",
    "            max_new_tokens=max_new_tokens_rollout,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        rollout_end_time = time.time()\n",
    "        print(f\"  Rollout 完成. 收集到 {len(all_rollout_data['responses'])} 条有效数据 (耗时: {rollout_end_time - epoch_start_time:.2f}s)。\")\n",
    "\n",
    "        print(f\"  开始计算 Rewards 和 GAE...\")\n",
    "        reward_gae_start_time = time.time()\n",
    "        reward_dict = get_reward_scores(\n",
    "            all_rollout_data, # Pass raw data\n",
    "            ref_model,\n",
    "            reward_model,\n",
    "            tokenizer,\n",
    "            reward_tokenizer,\n",
    "            kl_beta=kl_beta,\n",
    "            device=device,\n",
    "        )\n",
    "        (\n",
    "            rewards_batch, # Shape: (batch_size,)\n",
    "            padded_values_batch, # Shape: (batch_size, max_response_len + 1)\n",
    "            padded_policy_log_probs_batch, # Shape: (batch_size, max_response_len)\n",
    "            padded_response_ids_batch, # Shape: (batch_size, max_response_len)\n",
    "            padded_full_sequence_ids_batch, # Shape: (batch_size, max_total_len)\n",
    "            attention_mask_full_sequence_batch, # Shape: (batch_size, max_total_len)\n",
    "            response_mask_batch, # Shape: (batch_size, max_response_len)\n",
    "        ) = (\n",
    "            reward_dict['rewards'],\n",
    "            reward_dict['padded_values'],\n",
    "            reward_dict['padded_policy_log_probs'],\n",
    "            reward_dict['padded_response_ids'],\n",
    "            reward_dict['padded_full_sequence_ids'],\n",
    "            reward_dict['attention_mask_full_sequence'],\n",
    "            reward_dict['response_mask'],\n",
    "        )\n",
    "\n",
    "        reward_gae_step1_end_time = time.time()\n",
    "        print(f\"  Rewards 数据准备完成 (耗时: {reward_gae_step1_end_time - reward_gae_start_time:.2f}s)。\")\n",
    "\n",
    "        # 计算优势和回报\n",
    "        advantages_batch, returns_batch = calculate_gae(\n",
    "            rewards_batch, # Shape: (batch_size,)\n",
    "            padded_values_batch, # Shape: (batch_size, max_response_len + 1)\n",
    "            response_mask_batch, # Shape: (batch_size, max_response_len)\n",
    "            gamma=gamma,\n",
    "            lambda_=lambda_,\n",
    "            device=device\n",
    "        )\n",
    "        reward_gae_end_time = time.time()\n",
    "        print(f\"  GAE 计算完成 (耗时: {reward_gae_end_time - reward_gae_step1_end_time:.2f}s, 总计: {reward_gae_end_time - reward_gae_start_time:.2f}s)。\")\n",
    "        avg_rollout_reward = rewards_batch.mean().item() if rewards_batch.numel() > 0 else 0.0\n",
    "        print(f\"  平均 Rollout Reward: {avg_rollout_reward:.4f}\")\n",
    "\n",
    "        # PPO优化\n",
    "        optimization_start_time = time.time()\n",
    "\n",
    "        valid_indices = (response_mask_batch.sum(dim=1) > 0).cpu()\n",
    "        num_valid_sequences = valid_indices.sum().item()\n",
    "\n",
    "        ppo_dataset = PPODataset(\n",
    "            padded_policy_log_probs=padded_policy_log_probs_batch[valid_indices].cpu(),\n",
    "            padded_response_ids=padded_response_ids_batch[valid_indices].cpu(),\n",
    "            padded_full_sequence_ids=padded_full_sequence_ids_batch[valid_indices].cpu(),\n",
    "            attention_mask_full_sequence=attention_mask_full_sequence_batch[valid_indices].cpu(),\n",
    "            response_mask=response_mask_batch[valid_indices].cpu(),\n",
    "            advantages=advantages_batch[valid_indices].cpu(),\n",
    "            returns=returns_batch[valid_indices].cpu(),\n",
    "        )\n",
    "        ppo_dataloader = DataLoader(ppo_dataset, batch_size=minibatch_size, shuffle=True)\n",
    "        total_minibatches = len(ppo_dataloader)\n",
    "        if total_minibatches == 0:\n",
    "            continue\n",
    "\n",
    "        for ppo_epoch in range(ppo_epochs_per_rollout):\n",
    "            policy_model.train() #\n",
    "\n",
    "            total_loss_epoch = 0\n",
    "            policy_loss_epoch = 0\n",
    "            value_loss_epoch = 0\n",
    "            entropy_epoch = 0\n",
    "            minibatch_count = 0\n",
    "\n",
    "            ppo_epoch_start_time = time.time()\n",
    "\n",
    "            for i, minibatch in enumerate(ppo_dataloader):\n",
    "                (\n",
    "                    mb_padded_policy_log_probs, # Shape: (mb_size, max_response_len)\n",
    "                    mb_padded_response_ids, # Shape: (mb_size, max_response_len)\n",
    "                    mb_padded_full_sequence_ids, # Shape: (mb_size, max_total_len in this batch)\n",
    "                    mb_attention_mask_full_sequence, # Shape: (mb_size, max_total_len in this batch)\n",
    "                    mb_response_mask, # Shape: (mb_size, max_response_len)\n",
    "                    mb_advantages, # Shape: (mb_size, max_response_len)\n",
    "                    mb_returns, # Shape: (mb_size, max_response_len)\n",
    "                ) = [tensor.to(device) for tensor in minibatch]\n",
    "\n",
    "                minibatch_count += 1\n",
    "\n",
    "                mb_total_loss, mb_policy_loss, mb_value_loss, mb_entropy = calculate_ppo_loss(\n",
    "                    policy_model,\n",
    "                    mb_padded_policy_log_probs,\n",
    "                    mb_padded_response_ids,\n",
    "                    mb_padded_full_sequence_ids,\n",
    "                    mb_attention_mask_full_sequence,\n",
    "                    mb_response_mask,\n",
    "                    mb_advantages,\n",
    "                    mb_returns,\n",
    "                    clip_param=clip_param,\n",
    "                    vf_coef=vf_coef,\n",
    "                    ent_coef=ent_coef,\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                mb_total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss_epoch += mb_total_loss.item()\n",
    "                policy_loss_epoch += mb_policy_loss.item()\n",
    "                value_loss_epoch += mb_value_loss.item()\n",
    "                entropy_epoch += mb_entropy.item()\n",
    "\n",
    "            ppo_epoch_end_time = time.time()\n",
    "\n",
    "            if minibatch_count > 0:\n",
    "                avg_total_loss = total_loss_epoch / minibatch_count\n",
    "                avg_policy_loss = policy_loss_epoch / minibatch_count\n",
    "                avg_value_loss = value_loss_epoch / minibatch_count\n",
    "                avg_entropy = entropy_epoch / minibatch_count\n",
    "                print(f\"  PPO Epoch {ppo_epoch+1}/{ppo_epochs_per_rollout} 结束: Avg Total Loss={avg_total_loss:.4f}, Avg Policy Loss={avg_policy_loss:.4f}, Avg Value Loss={avg_value_loss:.4f}, Avg Entropy={avg_entropy:.4f} (耗时: {ppo_epoch_end_time - ppo_epoch_start_time:.2f}s)\")\n",
    "            else:\n",
    "                print(f\"  PPO Epoch {ppo_epoch+1}/{ppo_epochs_per_rollout} 结束: 没有有效 Minibatch.\")"
   ],
   "id": "7dfd23fe6b7c591a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:19:56.434801Z",
     "start_time": "2025-06-16T14:12:04.360409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_prompts_for_training = [\n",
    "        \"写一个关于机器学习的短介绍。\",\n",
    "        \"如何用Python实现一个简单的线性回归模型？\",\n",
    "        \"解释一下Transformer模型的自注意力机制。\",\n",
    "        \"PPO算法的核心思想是什么？\",\n",
    "        \"请举一个强化学习在实际中的应用例子。\",\n",
    "        \"神经网络中的激活函数有哪些？它们的作用是什么？\",\n",
    "        \"什么是过拟合和欠拟合？如何解决？\",\n",
    "        \"介绍一下卷积神经网络（CNN）的基本结构。\",\n",
    "        \"如何评估一个分类模型的性能？\",\n",
    "        \"写一段关于深度学习的未来展望。\",\n",
    "        \"描述一下梯度下降算法的工作原理。\",\n",
    "        \"请写一个关于数据清洗的步骤。\",\n",
    "        \"什么是迁移学习？它有什么优势？\",\n",
    "        \"生成一个简单的聊天对话。\",\n",
    "        \"推荐一本学习PyTorch的书籍。\",\n",
    "        \"解释一下GAN（生成对抗网络）的基本概念。\",\n",
    "        \"写一个关于自然语言处理（NLP）的应用例子。\",\n",
    "        \"如何在机器学习项目中选择合适的算法？\",\n",
    "        \"介绍一下决策树的工作原理。\",\n",
    "        \"写一段鼓励机器学习初学者的文字。\",\n",
    "         \"请提供一个关于自然语言处理的进阶概念解释。\",\n",
    "         \"如何使用BERT进行文本分类？\",\n",
    "         \"描述一下Seq2Seq模型及其在机器翻译中的应用。\",\n",
    "         \"什么是注意力机制（Attention Mechanism）？它解决了什么问题？\",\n",
    "         \"解释一下LSTM和GRU这两种循环神经网络。\",\n",
    "         \"推荐几个常用的自然语言处理工具库。\",\n",
    "         \"如何进行文本数据的预处理？\",\n",
    "         \"什么是词嵌入（Word Embedding）？\",\n",
    "         \"介绍一下预训练语言模型的常见架构（如BERT, GPT, RoBERTa）。\",\n",
    "         \"写一个关于情感分析的应用案例。\",\n",
    "] * 5\n",
    "\n",
    "trained_policy_model_wrapper = train_ppo(\n",
    "    policy_model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    tokenizer=tokenizer,\n",
    "    reward_tokenizer=reward_tokenizer,\n",
    "    all_prompts_dataset=all_prompts_for_training,\n",
    "    device=device,\n",
    "    epochs=3,\n",
    "    rollout_batch_size=2,\n",
    "    ppo_epochs_per_rollout=2,\n",
    "    minibatch_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    gamma=0.99,\n",
    "    lambda_=0.95,\n",
    "    clip_param=0.2,\n",
    "    vf_coef=0.5,\n",
    "    ent_coef=0.005,\n",
    "    kl_beta=0.1,\n",
    "    max_new_tokens_rollout=50, # Reduce max tokens for example\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")"
   ],
   "id": "d12d00f478a127b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/3 ---\n",
      "['什么是过拟合和欠拟合？如何解决？', '写一段关于深度学习的未来展望。']\n",
      "  Rollout 完成. 收集到 2 条有效数据 (耗时: 7.24s)。\n",
      "  开始计算 Rewards 和 GAE...\n",
      "  Rewards 数据准备完成 (耗时: 1.15s)。\n",
      "  GAE 计算完成 (耗时: 1.07s, 总计: 2.22s)。\n",
      "  平均 Rollout Reward: -13.7346\n",
      "  PPO Epoch 1/2 结束: Avg Total Loss=22.3315, Avg Policy Loss=0.3860, Avg Value Loss=43.9277, Avg Entropy=3.6694 (耗时: 84.94s)\n",
      "  PPO Epoch 2/2 结束: Avg Total Loss=13.6000, Avg Policy Loss=0.1256, Avg Value Loss=26.9861, Avg Entropy=3.7391 (耗时: 99.48s)\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "['如何评估一个分类模型的性能？', '如何使用BERT进行文本分类？']\n",
      "  Rollout 完成. 收集到 2 条有效数据 (耗时: 14.97s)。\n",
      "  开始计算 Rewards 和 GAE...\n",
      "  Rewards 数据准备完成 (耗时: 10.11s)。\n",
      "  GAE 计算完成 (耗时: 0.21s, 总计: 10.33s)。\n",
      "  平均 Rollout Reward: -7.5937\n",
      "  PPO Epoch 1/2 结束: Avg Total Loss=9.8873, Avg Policy Loss=0.1025, Avg Value Loss=19.5885, Avg Entropy=1.8939 (耗时: 104.07s)\n",
      "  PPO Epoch 2/2 结束: Avg Total Loss=4.7049, Avg Policy Loss=0.0488, Avg Value Loss=9.3304, Avg Entropy=1.8279 (耗时: 99.00s)\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "['介绍一下决策树的工作原理。', '介绍一下卷积神经网络（CNN）的基本结构。']\n",
      "  Rollout 完成. 收集到 2 条有效数据 (耗时: 18.17s)。\n",
      "  开始计算 Rewards 和 GAE...\n",
      "  Rewards 数据准备完成 (耗时: 11.18s)。\n",
      "  GAE 计算完成 (耗时: 0.18s, 总计: 11.36s)。\n",
      "  平均 Rollout Reward: -12.5259\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.08 GB, other allocations: 1.75 GB, max allowed: 18.13 GB). Tried to allocate 593.50 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 34\u001B[39m\n\u001B[32m      1\u001B[39m all_prompts_for_training = [\n\u001B[32m      2\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m写一个关于机器学习的短介绍。\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      3\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m如何用Python实现一个简单的线性回归模型？\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     31\u001B[39m          \u001B[33m\"\u001B[39m\u001B[33m写一个关于情感分析的应用案例。\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     32\u001B[39m ] * \u001B[32m5\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m trained_policy_model_wrapper = \u001B[43mtrain_ppo\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicy_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpolicy_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m    \u001B[49m\u001B[43mref_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mref_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreward_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreward_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreward_tokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreward_tokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m    \u001B[49m\u001B[43mall_prompts_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_prompts_for_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrollout_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m    \u001B[49m\u001B[43mppo_epochs_per_rollout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m    \u001B[49m\u001B[43mminibatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5e-6\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.99\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlambda_\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.95\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m    \u001B[49m\u001B[43mclip_param\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     50\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvf_coef\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m    \u001B[49m\u001B[43ment_coef\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.005\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkl_beta\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens_rollout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Reduce max tokens for example\u001B[39;49;00m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.9\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 160\u001B[39m, in \u001B[36mtrain_ppo\u001B[39m\u001B[34m(policy_model, ref_model, reward_model, tokenizer, reward_tokenizer, all_prompts_dataset, device, epochs, rollout_batch_size, ppo_epochs_per_rollout, minibatch_size, learning_rate, gamma, lambda_, clip_param, vf_coef, ent_coef, kl_beta, max_new_tokens_rollout, temperature, top_k, top_p, do_sample)\u001B[39m\n\u001B[32m    158\u001B[39m mb_total_loss.backward()\n\u001B[32m    159\u001B[39m torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=\u001B[32m0.5\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m160\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    162\u001B[39m total_loss_epoch += mb_total_loss.item()\n\u001B[32m    163\u001B[39m policy_loss_epoch += mb_policy_loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/myml/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    480\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    481\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    482\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    483\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m485\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    486\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    488\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/myml/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     77\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     78\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m79\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     80\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     81\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/myml/lib/python3.11/site-packages/torch/optim/adam.py:246\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    234\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    236\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    237\u001B[39m         group,\n\u001B[32m    238\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    243\u001B[39m         state_steps,\n\u001B[32m    244\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m246\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdecoupled_weight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    270\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/myml/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    146\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/myml/lib/python3.11/site-packages/torch/optim/adam.py:933\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    931\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m933\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    937\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    938\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    939\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    940\u001B[39m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    942\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/myml/lib/python3.11/site-packages/torch/optim/adam.py:525\u001B[39m, in \u001B[36m_single_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[39m\n\u001B[32m    523\u001B[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001B[32m    524\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m525\u001B[39m         denom = (\u001B[43mexp_avg_sq\u001B[49m\u001B[43m.\u001B[49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m).add_(eps)\n\u001B[32m    527\u001B[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001B[32m    529\u001B[39m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 16.08 GB, other allocations: 1.75 GB, max allowed: 18.13 GB). Tried to allocate 593.50 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T14:19:57.048032Z",
     "start_time": "2025-06-03T06:19:29.254092Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ac3286fca2ecd48c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
